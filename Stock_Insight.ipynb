{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out whether it's worth buying IBM stock. I don't know much about investing, but I'll try to break it down step by step.\n",
      "\n",
      "First, I remember that IBM is a big company, often called \"Big Blue.\" They've been around for a long time, so that might mean they're stable. I think they do a lot with computers and technology. But I'm not sure what exactly they're focused on now. Maybe I should look into their current business areas.\n",
      "\n",
      "I heard something about IBM splitting into two companies a while back. I think one part is IBM and the other is Kyndryl. So, what does IBM focus on now? Maybe they're more into services or cloud computing? I should check that.\n",
      "\n",
      "I also remember that IBM bought Red Hat a few years ago. Red Hat is known for Linux and open-source software, so maybe IBM is pushing into cloud and enterprise solutions. That could be a good area because cloud computing is growing.\n",
      "\n",
      "I should consider their financials. How has IBM been doing financially? Are they making profits, or are they struggling? I think they might have had some restructuring costs, which could affect their earnings. But if they're restructuring to focus on growth areas, that might be a positive sign.\n",
      "\n",
      "Looking at their stock performance, I wonder how IBM compares to others in the tech sector. Tech stocks can be volatile, but IBM is more established, so maybe it's less volatile than some others. I should compare it to others like Microsoft or Oracle.\n",
      "\n",
      "Dividends are another factor. I think IBM has been paying dividends for a long time, which might make it attractive for income investors. But I'm not sure how the dividend yield compares to others. I should look up the current yield and see if it's competitive.\n",
      "\n",
      "I also need to think about the overall market trends. Cloud computing is definitely a growing field, so if IBM is strong there, that's a plus. AI is another area I've heard IBM is involved in, especially with Watson. But I'm not sure how successful Watson has been in the market.\n",
      "\n",
      "R&D is important for tech companies. Is IBM investing enough in research and development? If they are, that could lead to innovation and future growth. But if they're cutting back, that might be a concern.\n",
      "\n",
      "Debt levels are something to consider too. If IBM has a lot of debt, that could be risky, especially if interest rates go up. I should check their debt-to-equity ratio and see how it compares to industry standards.\n",
      "\n",
      "The management team is another factor. I don't know much about IBM's current leadership, but if they have a strong, experienced team, that could be a good sign. I should look up who's running the company and their track record.\n",
      "\n",
      "I also need to assess the risks. What could go wrong? Maybe if their cloud business doesn't grow as expected, or if they face increased competition from other tech giants. Economic downturns could also affect their performance if businesses cut back on IT spending.\n",
      "\n",
      "I should also think about my own investment goals. Am I looking for long-term growth, or do I want dividend income? If I'm investing for the long haul, IBM's stability might be appealing. But if I'm looking for quick gains, maybe a different stock would be better.\n",
      "\n",
      "I'm not sure how to evaluate all these factors together. Maybe I should look up some recent analysis or financial reports on IBM. That could give me a better idea of their current standing and future prospects.\n",
      "\n",
      "In summary, I need to consider IBM's business focus, financial health, market position, dividend yield, R&D investment, debt levels, management, risks, and how they align with my investment goals. After gathering all this information, I can make a more informed decision on whether buying IBM stock is worth it for me.\n",
      "</think>\n",
      "\n",
      "Based on the thought process, here's a structured analysis to determine if buying IBM stock is worthwhile:\n",
      "\n",
      "### IBM Stock Analysis\n",
      "\n",
      "1. **Business Focus:**\n",
      "   - **Post-Spinoff Structure:** IBM split into IBM and Kyndryl, focusing on cloud, AI, and enterprise solutions. The acquisition of Red Hat indicates a strong push into cloud and open-source technologies, aligning with growing market trends.\n",
      "\n",
      "2. **Financial Health:**\n",
      "   - **Profitability:** Check recent financial reports for profitability, considering restructuring costs and their impact on earnings. Focus on whether restructuring is strategic for future growth.\n",
      "   - **Debt Levels:** Evaluate debt-to-equity ratio to assess financial risk, especially with rising interest rates.\n",
      "\n",
      "3. **Market Position:**\n",
      "   - **Cloud Computing:** IBM's focus on cloud services is a positive given the sector's growth. Assess how IBM's offerings compare to competitors like Microsoft and Oracle.\n",
      "   - **AI and R&D:** Consider IBM's investments in AI (e.g., Watson) and R&D to gauge innovation potential.\n",
      "\n",
      "4. **Stock Performance and Dividends:**\n",
      "   - **Stock Volatility:** As a established company, IBM may offer lower volatility compared to growth tech stocks.\n",
      "   - **Dividend Yield:** Compare IBM's dividend yield with industry peers to assess attractiveness for income investors.\n",
      "\n",
      "5. **Market Trends:**\n",
      "   - **Growth Areas:** Cloud computing and AI are key growth areas. Evaluate IBM's competitive position and market share in these sectors.\n",
      "\n",
      "6. **Management and Risks:**\n",
      "   - **Leadership:** Research the experience and track record of IBM's management team.\n",
      "   - **Risks:** Consider competition, economic downturn impacts on IT spending, and potential underperformance in cloud or AI ventures.\n",
      "\n",
      "7. **Investment Goals:**\n",
      "   - Align the investment with personal goals: long-term growth, dividend income, or short-term gains. IBM's stability may appeal more to long-term investors.\n",
      "\n",
      "### Conclusion:\n",
      "After evaluating IBM's business focus, financial health, market position, and alignment with personal investment goals, if the analysis indicates strong fundamentals and growth potential, buying IBM stock may be a prudent decision. However, this should be part of a diversified portfolio to mitigate risk. Consulting recent financial reports and analyses is recommended for up-to-date insights.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import getpass\n",
    "### Import Necessary LangChain Components\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import getpass\n",
    "\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "\n",
    "# Initialize LangChain's ChatGroq Model\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\",temperature=0.5)\n",
    "\n",
    "# Example Query\n",
    "response = llm.invoke(\"Please tell me it is worth buying IBM stock and provide me with some insights on the company.\")    \n",
    "print(response.content)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-deepseek (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-deepseek\n",
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'f:\\Project\\LLMStockPulse-main\\myvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatDeepSeek' from 'langchain.chat_models' (f:\\Project\\LLMStockPulse-main\\myvenv\\lib\\site-packages\\langchain\\chat_models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatDeepSeek  \u001b[38;5;66;03m# Import DeepSeek mode\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ChatDeepSeek' from 'langchain.chat_models' (f:\\Project\\LLMStockPulse-main\\myvenv\\lib\\site-packages\\langchain\\chat_models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatDeepSeek  # Import DeepSeek mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class ChatDeepSeek with abstract methods _generate, _llm_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatDeepSeek\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is LangChain?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\typing.py:873\u001b[0m, in \u001b[0;36mGeneric.__new__\u001b[1;34m(cls, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be instantiated; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit can be used only as a base class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m:\n\u001b[1;32m--> 873\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class ChatDeepSeek with abstract methods _generate, _llm_type"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.llms import BaseLLM\n",
    "import requests\n",
    "\n",
    "class ChatDeepSeek(BaseLLM):\n",
    "    def __init__(self, temperature=0.7):\n",
    "        self.temperature = temperature\n",
    "        self.api_key = os.environ.get(\"DEEPSEEK_API_KEY\")\n",
    "        self.endpoint = \"https://api.deepseek.ai/v1/chat\"  # Replace with actual DeepSeek endpoint\n",
    "    \n",
    "    def _call(self, prompt: str) -> str:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": \"chatgpt\",  # Specify the model you want\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        response = requests.post(self.endpoint, json=payload, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Example usage\n",
    "llm = ChatDeepSeek(temperature=0.5)\n",
    "\n",
    "response = llm(\"What is LangChain?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.48276466e-01 -3.67820472e-01  1.85436174e-01 -5.02136052e-01\n",
      "  6.02458954e-01  3.72113526e-01 -4.41867650e-01  3.83968204e-01\n",
      " -8.18194225e-02  3.17060039e-04 -3.18080306e-01  6.51477873e-02\n",
      " -4.97912429e-02  2.53298640e-01 -5.96161261e-02 -2.42521361e-01\n",
      "  6.79208159e-01 -5.30129373e-01  5.14001660e-02 -2.72889733e-01\n",
      " -5.24730086e-01 -1.76243648e-01  3.66427302e-01 -1.57511339e-01\n",
      "  1.82454899e-01 -3.53101283e-01 -1.92305416e-01  7.65354559e-02\n",
      " -1.01229839e-01 -1.11638813e-03 -2.72053570e-01  2.08752602e-01\n",
      "  1.17489502e-01  2.86106944e-01 -3.79813403e-01 -2.78891921e-01\n",
      "  4.18795526e-01  1.03510603e-01  4.77114357e-02  2.60568380e-01\n",
      " -3.33941262e-03 -6.08178079e-01 -1.44443791e-02 -1.44871220e-01\n",
      " -2.54031718e-01  1.07129879e-01 -2.60808039e-03  6.04766130e-01\n",
      " -2.23087575e-02  2.94883281e-01 -3.81902516e-01 -2.56321907e-01\n",
      "  2.01145425e-01  1.29524246e-01  5.54436684e-01 -3.93291980e-01\n",
      " -4.43785816e-01  6.78732097e-02  6.94405586e-02 -3.28105122e-01\n",
      "  1.56036913e-01 -5.44280648e-01 -1.47577494e-01  2.09341317e-01\n",
      "  5.49828172e-01  4.88121301e-01  3.82018030e-01  1.54049806e-02\n",
      " -3.25576931e-01 -1.90988332e-01  1.57397598e-01 -9.83054712e-02\n",
      "  4.90506977e-01  2.39831999e-01 -6.21894337e-02 -1.08562060e-01\n",
      "  2.75596321e-01 -2.59467572e-01 -2.47996282e-02 -3.20447654e-01\n",
      "  2.28995547e-01 -7.56526947e-01 -2.95502841e-01 -4.90285546e-01\n",
      "  3.76086980e-01 -2.87258625e-01 -2.86813471e-02 -2.43886471e-01\n",
      " -4.28362042e-01 -3.12378049e-01  3.31577063e-02 -2.38696977e-01\n",
      "  2.29341805e-01  1.31436020e-01  2.79180467e-01  5.29885948e-01\n",
      " -2.01116592e-01 -1.45001993e-01 -8.83702710e-02  8.58721286e-02\n",
      "  2.87979320e-02 -2.06819460e-01  5.48991621e-01  5.16704544e-02\n",
      "  1.50407478e-01 -4.03884768e-01 -1.66201353e-01  5.62287867e-01\n",
      " -2.13179126e-01  2.84228384e-01 -1.60427973e-01  4.03742231e-02\n",
      " -2.05903217e-01 -4.06026781e-01  2.20007569e-01 -6.61235571e-01\n",
      "  4.83279824e-01  3.05272549e-01 -2.30213076e-01 -8.09794739e-02\n",
      "  8.49081352e-02  6.10601716e-02 -2.09524453e-01  4.01814803e-02\n",
      " -5.25777519e-01  4.50501233e-01  9.11943763e-02  3.12923700e-01\n",
      "  3.72142762e-01 -2.50953585e-01  2.39600524e-01 -1.83745086e-01\n",
      " -5.25423765e-01  1.24424733e-01 -7.68749595e-01 -2.31498316e-01\n",
      "  4.89223659e-01 -2.65516937e-01 -1.49249882e-01  2.11924631e-02\n",
      " -6.73186541e-01  1.75979868e-01 -2.90727057e-02 -4.38496560e-01\n",
      "  5.44080101e-02  3.03094834e-01 -2.97678471e-01  1.12506278e-01\n",
      " -6.04742132e-02  3.59279484e-01  2.78826267e-01 -5.98873436e-01\n",
      " -1.11623816e-01  3.02109629e-01  3.91386181e-01 -4.81487483e-01\n",
      "  5.60404100e-02  2.22446144e-01  3.72090340e-01  2.59207785e-01\n",
      "  1.62873581e-01 -2.23711103e-01  2.96938956e-01 -2.15968356e-01\n",
      " -3.10976535e-01 -1.97658062e-01  4.30015236e-01  1.07209846e-01\n",
      " -6.81111738e-02  8.39634333e-03 -3.49993020e-01  4.51621234e-01\n",
      "  6.27873957e-01  2.29139537e-01 -5.61922975e-02 -6.38182685e-02\n",
      " -2.91037969e-02 -1.93311438e-01  1.70778170e-01 -4.44296859e-02\n",
      " -9.58030820e-01 -4.68174338e-01  1.51509896e-01  1.93402663e-01\n",
      "  1.77069351e-01 -3.81325245e-01  2.61906117e-01  5.22900037e-02\n",
      "  7.72904083e-02 -1.79058075e-01 -1.12794498e-02 -7.52198875e-01\n",
      " -5.38589396e-02  3.87668490e-01  3.79835427e-01 -9.99198854e-02\n",
      " -4.34066445e-01  3.71693909e-01  2.92848311e-02  3.04229945e-01\n",
      " -4.06699717e-01  1.40029997e-01  3.84619907e-02  1.48002058e-01\n",
      "  3.09609413e-01 -2.58600682e-01 -1.24483757e-01  2.13471889e-01\n",
      " -6.33639842e-02  1.07359083e-03  1.81551620e-01  2.18021095e-01\n",
      "  1.60272419e-01  3.68319899e-02  2.53283560e-01  1.97023094e-01\n",
      "  3.61076683e-01  1.43893078e-01 -5.80716491e-01  1.98842455e-02\n",
      "  4.25470144e-01  1.51561365e-01  3.16278160e-01 -3.71681005e-01\n",
      " -8.16203713e-01  2.01802701e-01 -6.67912781e-01  5.65575838e-01\n",
      "  4.82831806e-01  1.20552525e-01 -1.07599497e-01  7.78212547e-01\n",
      "  1.11682013e-01  3.97382230e-02  1.58018768e-01 -2.62665302e-01\n",
      " -2.15440497e-01  1.13058165e-01 -2.99080051e-02  4.40916717e-01\n",
      "  3.46268177e-01  1.43122226e-01 -5.21936476e-01  8.91034529e-02\n",
      " -6.13929182e-02 -1.35636508e-01 -1.22142923e+00  6.47439361e-01\n",
      " -4.36408401e-01  1.08451098e-01 -4.75009620e-01  2.27122277e-01\n",
      "  2.61820942e-01 -4.90390182e-01 -7.40343854e-02  3.99574250e-01\n",
      " -7.84932449e-03  1.28069386e-01  3.39174718e-01  3.92185062e-01\n",
      "  1.03396863e-01 -6.75543845e-01 -1.31418243e-01  1.71031341e-01\n",
      "  5.89167535e-01 -2.01896906e-01 -3.75317276e-01  1.65015548e-01\n",
      " -4.90528226e-01  2.30844155e-01 -7.00551331e-01  4.03669983e-01\n",
      " -4.51608837e-01 -2.65866816e-01  1.48277104e-01  2.67399013e-01\n",
      " -4.52288128e-02 -1.89622492e-01 -2.89322883e-01  2.48509377e-01\n",
      " -4.81989086e-01  3.93924676e-02  2.17522845e-01  3.11011791e-01\n",
      " -9.59324241e-01  3.06743264e-01 -1.15341589e-01  1.77177683e-01\n",
      " -1.24683969e-01 -5.65098338e-02  3.12584609e-01  1.72091797e-01\n",
      " -1.71254128e-01 -2.56160468e-01  2.66430020e-01  2.71056853e-02\n",
      " -3.19444686e-01 -6.51083529e-01 -1.36851341e-01 -1.81274578e-01\n",
      "  1.06513254e-01  6.54997230e-01  4.93892491e-01  2.16310173e-01\n",
      " -2.14199632e-01  6.06123984e-01 -1.62637718e-02 -1.27286881e-01\n",
      " -4.64947581e-01  6.52160287e-01  6.20109320e-01 -3.16339701e-01\n",
      "  1.25212083e-02 -4.26746905e-01 -4.31032449e-01  1.04881778e-01\n",
      " -3.75806630e-01 -1.76134825e-01  1.21799251e-02 -5.93922548e-02\n",
      "  1.55241996e-01  4.01780933e-01  1.79416016e-01  4.16269958e-01\n",
      "  8.55427980e-02  3.34879428e-01  4.27725613e-01  4.52835411e-01\n",
      "  2.18847170e-01  2.20976979e-01 -5.25315925e-02 -2.06095785e-01\n",
      " -3.25412154e-01  9.21287201e-03  4.45480376e-01 -1.36785895e-01\n",
      " -1.21994004e-01 -1.72919884e-01  3.23846471e-03  3.61601681e-01\n",
      "  2.47695699e-01  4.46332812e-01  7.63319850e-01 -1.06318153e-01\n",
      "  2.07849026e-01  1.16495118e-01 -3.18061411e-01  2.35810012e-01\n",
      "  4.61634845e-01  9.36997414e-01  1.43993124e-01 -8.62634480e-02\n",
      " -4.01227683e-01 -7.26056814e-01 -3.04724872e-01  1.74928173e-01\n",
      " -6.04930893e-02  1.98264420e-01 -1.70469925e-01 -6.14859343e-01\n",
      "  9.93689954e-01 -1.23161629e-01  6.18422508e-01  8.00180674e-01\n",
      "  1.96318254e-01 -2.11929470e-01  3.54301602e-01 -3.57380033e-01\n",
      "  2.51677871e-01 -3.77469271e-01 -3.49448353e-01 -6.32939637e-01\n",
      "  6.03923082e-01 -4.89059776e-01 -3.84837426e-02  1.04795940e-01\n",
      "  5.78917712e-02 -3.93011272e-01 -7.62359649e-02 -1.25297636e-01\n",
      "  2.16945447e-02 -8.67515326e-01 -5.76771438e-01  4.90150481e-01]\n",
      "[[-0.29235494 -0.44723517 -0.06734492 ... -0.4892108  -0.01431178\n",
      "   0.47836494]\n",
      " [ 0.11345863 -0.02121071 -0.32665467 ... -0.75276744  0.19353242\n",
      "   0.26013988]\n",
      " [ 0.20547874 -0.09621891 -0.43790278 ... -0.4141174   0.1291918\n",
      "   0.11010495]\n",
      " [ 0.44280034 -0.14621942 -0.26326722 ... -0.29203382  0.3283254\n",
      "  -0.03247078]\n",
      " [ 0.01941047 -0.4162996  -0.638156   ... -0.27446616  0.3764019\n",
      "  -0.31874445]]\n",
      "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000222BF9337B0> >\n",
      "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x00000222BF9337B0> >\n",
      "[[29.013294 48.37797  53.16239 ]] [[0 2 3]]\n",
      "[{'url': 'https://apnews.com/hub/trending-news', 'content': '‘It looks like a stream of blood.’ A river near Buenos Aires turns red, sparking fears of toxic leak Nielsen says Christmas had more viewing on streaming services than any day ever Buffalo Bills quarterback Josh Allen, fiancee Hailee Steinfeld walk red carpet at NFL Honors ‘Wicked’ star Cynthia Erivo is feted as Harvard’s Hasty Pudding Woman of the Year Trump says he’s given advisers instructions for Iran to be ‘obliterated’ if it assassinates him Controlling owner Josh Harris says the Washington Commanders name is not changing ‘Mad Men’ star Jon Hamm honored as Harvard’s Hasty Pudding Man of the Year Trump was challenged after blaming DEI for the DC plane crash. Who is Sean Duffy, the new transportation secretary responding to the DC plane crash?'}, {'url': 'https://www.usnews.com/news/world', 'content': \"World News Headlines - US News and World Report News News Health News Business News Rankings Index News FAQ Rankings Index News FAQ News FAQ Photos U.S. News Live World News News World News World News ### Mexico, Canada Tariffs Coming Tuesday, but Trump Will Set Exact Levels, Says US Commerce Head ReutersMarch 2, 2025 Trump Aide Waltz Says U.S. Needs Ukrainian Leader Who Wants Peace Photos Photos Photos: President Trump's First Days Photos Rankings Index Countries News Countries FAQ Rankings Index States News States FAQ News Health News U.S. News Decision Points Photos News U.S. News Decision Points Photos About U.S. News Copyright 2025 © U.S. News & World Report L.P.Terms & Conditions/Privacy Policy/U.S. State Privacy Notice/Your Privacy Choices \"}, {'url': 'https://www.bbc.com/news/world', 'content': '5 hrs ago World 3 hrs ago World 5 hrs ago World 11 hrs ago World 2 hrs ago  Judge pauses Trump plan to put thousands of USAID staff on leave ---------------------------------------------------------------- Only about 600 out of 10,000 employees would have kept working at the agency under the president\\'s plan. 2 hrs ago  Trump says he is revoking Biden\\'s security clearance ---------------------------------------------------- \"There is no need for Joe Biden to continue receiving access to classified information,\" President Donald Trump says. 4 hrs ago  Tourists flee and homes break apart in Santorini, but resilient locals remain ----------------------------------------------------------------------------- People started evacuating the island as it was shook by tremors - but not everyone is fleeing. About the BBC'}]\n",
      "Here are some news headlines extracted from the provided information:\n",
      "\n",
      "1. River near Buenos Aires turns red, sparking fears of toxic leak\n",
      "2. Nielsen says Christmas had more viewing on streaming services than any day ever\n",
      "3. Buffalo Bills quarterback Josh Allen, fiancee Hailee Steinfeld walk red carpet at NFL Honors\n",
      "4. 'Wicked' star Cynthia Erivo is feted as Harvard’s Hasty Pudding Woman of the Year\n",
      "5. Trump says he’s given advisers instructions for Iran to be ‘obliterated’ if it assassinates him\n",
      "6. Controlling owner Josh Harris says the Washington Commanders name is not changing\n",
      "7. 'Mad Men' star Jon Hamm honored as Harvard’s Hasty Pudding Man of the Year\n",
      "8. Trump was challenged after blaming DEI for the DC plane crash. Who is Sean Duffy, the new transportation secretary responding to the DC plane crash?\n",
      "9. Mexico, Canada Tariffs Coming Tuesday, but Trump Will Set Exact Levels, Says US Commerce Head\n",
      "10. Trump Aide Waltz Says U.S. Needs Ukrainian Leader Who Wants Peace\n",
      "11. Judge pauses Trump plan to put thousands of USAID staff on leave\n",
      "12. Trump says he is revoking Biden’s security clearance\n",
      "13. Tourists flee and homes break apart in Santorini, but resilient locals remain\n",
      "\n",
      "Please note that some of the headlines may be incomplete or require additional context to fully understand.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss  # For fast similarity search\n",
    "\n",
    "# Ensure the API Key is set\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "\n",
    "# Initialize LangChain's ChatGroq Model\n",
    "llm = ChatGroq(temperature=0)\n",
    "\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")\n",
    "    \n",
    "def process_search_tool(url: str) -> str:\n",
    "    \"\"\"Fetches and extracts content from a given URL.\"\"\"\n",
    "    try:\n",
    "        # Add headers to mimic a real browser\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract text from specific tags to avoid noise\n",
    "        text = \" \".join([p.get_text() for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"article\"])])\n",
    "        return text\n",
    "    except requests.Timeout:\n",
    "        return f\"Timeout error while processing URL {url}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing URL {url}: {str(e)}\"\n",
    "\n",
    "# Asynchronous function to process multiple URLs concurrently\n",
    "async def process_multiple_urls(urls):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Use partial to pass the function and its arguments\n",
    "    tasks = [loop.run_in_executor(None, partial(process_search_tool, url)) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Sentence-BERT Model for Embedding Generation\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Example Query to Encode\n",
    "query = \"Tell me top ten trending current news in the world\"\n",
    "\n",
    "# Generate Embedding for Query\n",
    "query_embedding = sbert_model.encode([query])[0]\n",
    "print(query_embedding)\n",
    "\n",
    "def tavily_search(query: str) -> list:\n",
    "    \"\"\"Fetches search results for a given query using Tavily.\"\"\"\n",
    "    tavily_search = TavilySearchResults(max_results=5, search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True)\n",
    "    return tavily_search.run(query)\n",
    "\n",
    "# Example tool use for searching data\n",
    "search_results = tavily_search(query)\n",
    "\n",
    "# Use the retrieved search results to encode their embeddings\n",
    "search_embeddings = sbert_model.encode(search_results)\n",
    "print(search_embeddings)\n",
    "\n",
    "# Now, let's set up FAISS for fast similarity search\n",
    "index = faiss.IndexFlatL2(query_embedding.shape[0])  # Using L2 distance (Euclidean distance)\n",
    "print(index)\n",
    "index.add(np.array(search_embeddings))  # Add search embeddings to the FAISS index\n",
    "print(index)\n",
    "\n",
    "# Perform the similarity search (retrieve top-k most similar search results)\n",
    "k = 3  # Number of similar results you want to fetch\n",
    "D, I = index.search(np.array([query_embedding]), k)  # D: distances, I: indices of nearest neighbors\n",
    "print(D, I)\n",
    "\n",
    "# Fetch the most relevant results based on the indices\n",
    "relevant_results = [search_results[i] for i in I[0]]\n",
    "print(relevant_results)\n",
    "\n",
    "# Now you can integrate these results as context to refine the query\n",
    "query_with_context = f\"Based on this information: {relevant_results}\"\n",
    "\n",
    "# Run the refined query with additional context through ChatGroq\n",
    "response_with_context = llm.invoke(query_with_context)\n",
    "print(response_with_context.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that there was a heated argument between President Donald Trump, Ukrainian President Volodymyr Zelensky, and Vice President JD Vance during a meeting in the White House Oval Office on February 28, 2025. The argument was significant enough to cause a pause in intelligence sharing between the US and Ukraine, according to Trump officials.\n",
      "\n",
      "President Zelensky has since described the meeting as \"regrettable\" and noted that Ukraine is ready to negotiate about an end to the conflict. The exact reason for the argument is not specified in the articles, but it is mentioned that it was a \"blowup\" and that the meeting \"did not go the way it was supposed to.\"\n",
      "\n",
      "There are also several images attached to one of the articles showing President Trump and President Zelensky during the meeting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss  # For fast similarity search\n",
    "import tiktoken\n",
    "\n",
    "# Ensure the API Key is set\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "\n",
    "# Initialize LangChain's ChatGroq Model\n",
    "llm = ChatGroq(temperature=0)\n",
    "\n",
    "# Ensure Tavily API Key is set\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")\n",
    "\n",
    "# Initialize Sentence-BERT Model for Embedding Generation\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Tavily Search Tool\n",
    "tavily_tool = TavilySearchResults(max_results=5, search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True)\n",
    "\n",
    "# Example Query\n",
    "query = \"Tell me what happened between Donald Trump and Zelensky at the Oval office meeting?\"\n",
    "\n",
    "# Step 1: Fetch search results using Tavily\n",
    "search_results = tavily_tool.run(query)\n",
    "\n",
    "# Step 2: Extract content from search results\n",
    "search_contents = [result.get(\"content\", \"\") for result in search_results]\n",
    "\n",
    "# Step 2: Generate embeddings for the query and search results\n",
    "query_embedding = sbert_model.encode([query])[0]  # Embedding for the query\n",
    "search_embeddings = sbert_model.encode([result[\"content\"] for result in search_results])  # Embeddings for search results\n",
    "\n",
    "# Step 3: Set up FAISS for similarity search\n",
    "index = faiss.IndexFlatL2(query_embedding.shape[0])  # Using L2 distance (Euclidean distance)\n",
    "index.add(np.array(search_embeddings))  # Add search embeddings to the FAISS index\n",
    "\n",
    "# Step 4: Perform similarity search (retrieve top-k most similar search results)\n",
    "k = 3  # Number of similar results to fetch\n",
    "D, I = index.search(np.array([query_embedding]), k)  # D: distances, I: indices of nearest neighbors\n",
    "\n",
    "# Step 5: Fetch the most relevant results based on the indices\n",
    "relevant_results = [search_results[i] for i in I[0]]\n",
    "\n",
    "# Step 6: Refine the query with the relevant context\n",
    "query_with_context = f\"Based on this information: {relevant_results}\"\n",
    "\n",
    "# Step 7: Check token count before sending the request\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "total_tokens = len(encoding.encode(query_with_context))\n",
    "\n",
    "if total_tokens > 5000:\n",
    "    print(\"Request too large. Reducing input size.\")\n",
    "    relevant_results = [result[:100] for result in relevant_results]  # Further trim results\n",
    "    query_with_context = f\"Based on this information: {relevant_results}\"\n",
    "\n",
    "# Step 8: Run the refined query with additional context through ChatGroq\n",
    "response_with_context = llm.invoke(query_with_context)\n",
    "print(response_with_context.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp38-cp38-win_amd64.whl (798 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\project\\llmstockpulse-main\\myvenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'f:\\Project\\LLMStockPulse-main\\myvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 302\u001b[0m\n\u001b[0;32m    299\u001b[0m tools \u001b[38;5;241m=\u001b[39m [get_news_from_newsapi,get_market_sentiment_news,get_stock_data,get_stock_analysis,tavily_rag_search, process_search_tool]\n\u001b[0;32m    300\u001b[0m \u001b[38;5;66;03m### Create a Chat Prompt Template\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Step 1: Define Summarized Memory\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryMemory(llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm\u001b[49m, memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Step 2: Update the Prompt to Use Summarized Memory\u001b[39;00m\n\u001b[0;32m    305\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m    306\u001b[0m     [\n\u001b[0;32m    307\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     ]\n\u001b[0;32m    315\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "### Import Necessary LangChain Components\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Add this import at the top with other imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "API_KEY =\"YL41PNDL63AAWZOI\"\n",
    "\n",
    "@tool(\"stock_api_tool\", return_direct=False)\n",
    "def get_stock_data(stock_symbol: str, data_type: str = \"intraday\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches stock data for a given stock symbol.\n",
    "    \n",
    "    Available data_type options:\n",
    "    - \"intraday\": Latest stock price (1-minute interval)\n",
    "    - \"daily\": Daily adjusted closing prices\n",
    "    - \"fundamental\": Company overview (market cap, EPS, PE ratio)\n",
    "    - \"indicators\": Technical indicators (RSI, MACD)\n",
    "    \"\"\"\n",
    "    \n",
    "    API_KEY = \"QYBCUX9XUW8ESTIU\"\n",
    "    BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "    params = {\"symbol\": stock_symbol, \"apikey\": API_KEY}\n",
    "\n",
    "    if data_type == \"intraday\":\n",
    "        params[\"function\"] = \"TIME_SERIES_INTRADAY\"\n",
    "        params[\"interval\"] = \"1min\"\n",
    "    elif data_type == \"daily\":\n",
    "        params[\"function\"] = \"TIME_SERIES_DAILY_ADJUSTED\"\n",
    "    elif data_type == \"fundamental\":\n",
    "        params[\"function\"] = \"OVERVIEW\"\n",
    "    elif data_type == \"indicators\":\n",
    "        params[\"function\"] = \"RSI\"\n",
    "        params[\"interval\"] = \"daily\"\n",
    "        params[\"time_period\"] = \"14\"\n",
    "        params[\"series_type\"] = \"close\"\n",
    "    elif data_type == \"financials\":\n",
    "        params[\"function\"] = \"INCOME_STATEMENT\"\n",
    "    else:\n",
    "        return \"Invalid data type. Choose from 'intraday', 'daily', 'fundamental', 'financials','market_sentiment_news', or 'indicators'.\"\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    try:\n",
    "        if data_type == \"intraday\":\n",
    "            latest_data = data['Time Series (1min)']\n",
    "            latest_timestamp = next(iter(latest_data))\n",
    "            stock_info = latest_data[latest_timestamp]\n",
    "            return f\"Stock: {stock_symbol} - Open: {stock_info['1. open']}, High: {stock_info['2. high']}, Low: {stock_info['3. low']}, Close: {stock_info['4. close']}, Volume: {stock_info['5. volume']} at {latest_timestamp}\"\n",
    "        \n",
    "        elif data_type == \"daily\":\n",
    "            latest_data = data['Time Series (Daily)']\n",
    "            latest_date = next(iter(latest_data))\n",
    "            stock_info = latest_data[latest_date]\n",
    "            return f\"Stock: {stock_symbol} - Open: {stock_info['1. open']}, High: {stock_info['2. high']}, Low: {stock_info['3. low']}, Close: {stock_info['4. close']}, Adjusted Close: {stock_info['5. adjusted close']}, Volume: {stock_info['6. volume']} on {latest_date}\"\n",
    "        \n",
    "        elif data_type == \"fundamental\":\n",
    "            return f\"Company: {data['Name']} ({stock_symbol})\\nMarket Cap: {data['MarketCapitalization']}\\nEPS: {data['EPS']}\\nPE Ratio: {data['PERatio']}\\nDividend Yield: {data['DividendYield']}\\nSector: {data['Sector']}\"\n",
    "        \n",
    "        elif data_type == \"indicators\":\n",
    "            rsi_data = data['Technical Analysis: RSI']\n",
    "            latest_date = next(iter(rsi_data))\n",
    "            return f\"Stock: {stock_symbol} - RSI: {rsi_data[latest_date]['RSI']} on {latest_date}\"\n",
    "        elif data_type == \"financials\":\n",
    "            annual_reports = data[\"annualReports\"][0]  # Latest financial year report\n",
    "            return f\"Company: {stock_symbol}\\nRevenue: {annual_reports['totalRevenue']}\\nNet Income: {annual_reports['netIncome']}\\nProfit Margin: {annual_reports['grossProfit']}\"\n",
    "\n",
    "\n",
    "    except KeyError:\n",
    "        return \"Error fetching stock data. Check API limits or verify the stock symbol.\"\n",
    "    \n",
    "@tool(\"stock_news_api_tool\", return_direct=False)\n",
    "def get_market_sentiment_news(ticker: str = None, topics: str = None):\n",
    "    \"\"\"\n",
    "    Fetches the latest market sentiment news for a given stock ticker or market topics.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker (str, optional): Stock symbol (e.g., \"AAPL\") to fetch news for.\n",
    "    - topics (str, optional): Market topics such as \"Earnings\", \"IPO\", etc.\n",
    "\n",
    "    Returns:\n",
    "    - str: Formatted string containing the top 3 news articles with title, source, sentiment, and URL.\n",
    "    \"\"\"\n",
    "\n",
    "    API_KEY = \"QYBCUX9XUW8ESTIU\"\n",
    "    BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "\n",
    "    if ticker:\n",
    "        params[\"tickers\"] = ticker  # Fetch news specific to the stock\n",
    "    if topics:\n",
    "        params[\"topics\"] = topics  # Filter by topics like Earnings, IPO, etc.\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    news_items = data.get(\"feed\", [])\n",
    "    if not news_items:\n",
    "        return \"No market news available.\"\n",
    "\n",
    "    # Extract top 3 news articles\n",
    "    news_summary = []\n",
    "    for news in news_items[:3]:\n",
    "        news_summary.append(\n",
    "            f\"**Title**: {news['title']}\\n\"\n",
    "            f\"**Source**: {news['source']}\\n\"\n",
    "            f\"**Sentiment**: {news['overall_sentiment_label']}\\n\"\n",
    "            f\"**URL**: {news['url']}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(news_summary)\n",
    "\n",
    "@tool(\"process_search_tool\", return_direct=False)\n",
    "def process_search_tool(url: str) -> str:\n",
    "    \"\"\"Fetches and extracts content from a given URL.\"\"\"\n",
    "    try:\n",
    "        # Add headers to mimic a real browser\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract text from specific tags to avoid noise\n",
    "        text = \" \".join([p.get_text() for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"article\"])])\n",
    "        return text\n",
    "    except requests.Timeout:\n",
    "        return f\"Timeout error while processing URL {url}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing URL {url}: {str(e)}\"\n",
    "\n",
    "# Asynchronous function to process multiple URLs concurrently\n",
    "async def process_multiple_urls(urls):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Use partial to pass the function and its arguments\n",
    "    tasks = [loop.run_in_executor(None, partial(process_search_tool, url)) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Function to fetch and return up to 5 search results from Tavily\n",
    "@tool(\"tavily_search_tool\", return_direct=False)\n",
    "def tavily_rag_search(query: str) -> list:\n",
    "    \"\"\"Enhanced search with RAG processing\"\"\"\n",
    "    tavily = TavilySearchResults(\n",
    "        max_results=5,\n",
    "        search_depth=\"advanced\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True\n",
    "    )\n",
    "    results = tavily.run(query)\n",
    "# Convert to LangChain Documents\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=res[\"content\"],\n",
    "            metadata={\"source\": res[\"url\"], \"title\": res.get(\"title\", \"\")}\n",
    "        ) for res in results\n",
    "    ]\n",
    "    \n",
    "    # Rest of your RAG processing\n",
    "    search_contents = [doc.page_content for doc in documents]\n",
    "    query_embedding = sbert_model.encode([query])[0]\n",
    "    search_embeddings = sbert_model.encode(search_contents)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(query_embedding.shape[0])\n",
    "    index.add(np.array(search_embeddings))\n",
    "    \n",
    "    k = 3\n",
    "    D, I = index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    relevant_docs = [documents[i] for i in I[0]]  # Return Document objects\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "\n",
    "# NewsAPI tool to fetch news articles\n",
    "@tool(\"news_api_tool\", return_direct=False)\n",
    "def get_news_from_newsapi(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the latest news articles from NewsAPI for a specific query.\n",
    "    \"\"\"\n",
    "    API_KEY = \"d2afe10169b44e628b2131aed04ac7e4\"  # Add your NewsAPI key here\n",
    "    BASE_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,  # Use the query from Tavily search\n",
    "        \"apiKey\": API_KEY,\n",
    "        \"language\": \"en\",  # You can adjust the language as needed\n",
    "        \"sortBy\": \"relevance\",  # Sort by relevance or any other criteria\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract top 3 news articles\n",
    "    if data.get(\"status\") == \"ok\":\n",
    "        articles = data.get(\"articles\", [])\n",
    "        if articles:\n",
    "            news_summary = []\n",
    "            for article in articles[:3]:  # Limit to top 3 articles\n",
    "                news_summary.append(\n",
    "                    f\"**Title**: {article['title']}\\n\"\n",
    "                    f\"**Source**: {article['source']['name']}\\n\"\n",
    "                    f\"**Description**: {article['description']}\\n\"\n",
    "                    f\"**URL**: {article['url']}\\n\"\n",
    "                )\n",
    "            return \"\\n\".join(news_summary)\n",
    "    return \"No news articles found.\"\n",
    "\n",
    "\n",
    "@tool(\"get_stock_analysis_tool\", return_direct=False)\n",
    "def get_stock_analysis(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches stock financial data, technical indicators, and news sentiment analysis \n",
    "    for a given stock symbol and provides a Buy/Hold/Sell recommendation.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch stock overview (P/E ratio, market cap, dividend yield)\n",
    "        overview_url = f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={API_KEY}\"\n",
    "        stock_data = requests.get(overview_url).json()\n",
    "\n",
    "        if \"Error Message\" in stock_data or \"Note\" in stock_data:\n",
    "            return f\"❌ Error: Could not fetch data for {symbol}. API limit may be reached.\"\n",
    "\n",
    "        pe_ratio = float(stock_data.get(\"PERatio\", 0))\n",
    "        market_cap = float(stock_data.get(\"MarketCapitalization\", 0))\n",
    "        dividend_yield = float(stock_data.get(\"DividendYield\", 0))\n",
    "\n",
    "        # Fetch RSI (Relative Strength Index)\n",
    "        rsi_url = f\"https://www.alphavantage.co/query?function=RSI&symbol={symbol}&interval=daily&time_period=14&series_type=close&apikey={API_KEY}\"\n",
    "        rsi_data = requests.get(rsi_url).json()\n",
    "        rsi_values = rsi_data.get(\"Technical Analysis: RSI\", {})\n",
    "\n",
    "        # Get latest available RSI value\n",
    "        latest_rsi_date = next(iter(rsi_values), None)\n",
    "        rsi_value = float(rsi_values[latest_rsi_date][\"RSI\"]) if latest_rsi_date else 50  # Default 50 if unavailable\n",
    "\n",
    "        # Fetch latest stock price\n",
    "        price_url = f\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey={API_KEY}\"\n",
    "        price_data = requests.get(price_url).json()\n",
    "        stock_price = float(price_data.get(\"Global Quote\", {}).get(\"05. price\", 0))\n",
    "\n",
    "        # Fetch News Sentiment\n",
    "        news_url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={API_KEY}\"\n",
    "        news_response = requests.get(news_url).json()\n",
    "        news_sentiment = news_response.get(\"feed\", [])\n",
    "\n",
    "        # Compute average sentiment score from top 5 news articles\n",
    "        total_sentiment = sum(news.get(\"overall_sentiment_score\", 0) for news in news_sentiment[:5])\n",
    "        avg_sentiment = total_sentiment / max(len(news_sentiment[:5]), 1)  # Avoid division by zero\n",
    "\n",
    "        # **Decision Logic**\n",
    "        if rsi_value < 30 and pe_ratio < 20 and avg_sentiment > 0:\n",
    "            recommendation = \"🔵 BUY: The stock is undervalued and news sentiment is positive.\"\n",
    "        elif rsi_value > 70 and avg_sentiment < 0:\n",
    "            recommendation = \"🔴 SELL: The stock is overbought and news sentiment is negative.\"\n",
    "        else:\n",
    "            recommendation = \"🟡 HOLD: Market conditions are stable.\"\n",
    "\n",
    "        return f\"\"\"\n",
    "        📈 **Stock Analysis for {symbol}**\n",
    "        - **Current Price:** ${stock_price:.2f}\n",
    "        - **P/E Ratio:** {pe_ratio:.2f}\n",
    "        - **Market Cap:** ${market_cap:,.0f}\n",
    "        - **RSI (14-day):** {rsi_value:.2f}  \n",
    "        - **Dividend Yield:** {dividend_yield:.2%}\n",
    "        - **News Sentiment Score:** {avg_sentiment:.2f}\n",
    "\n",
    "        **Recommendation: {recommendation}**\n",
    "        \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error fetching data: {str(e)}\"\n",
    "\n",
    "tools = [get_news_from_newsapi,get_market_sentiment_news,get_stock_data,get_stock_analysis,tavily_rag_search, process_search_tool]\n",
    "### Create a Chat Prompt Template\n",
    "# Step 1: Define Summarized Memory\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Step 2: Update the Prompt to Use Summarized Memory\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Use the tavily_search_results_json tool for information.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),  # Summarized memory placeholder\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3: Construct the Agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Step 4: Create an Agent Executor with Summarized Memory\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)\n",
    "\n",
    "# Example Queries with Summarization Memory\n",
    "query1 = \"Tell me the stock market performance in NSE?\"\n",
    "response1 = agent_executor.invoke({\"input\": query1})\n",
    "print(response1[\"output\"])\n",
    "\n",
    "query2 = \"Give me an update on the same market today.\"\n",
    "response2 = agent_executor.invoke({\"input\": query2})\n",
    "print(response2[\"output\"])  # Now considers summarized past conversations\n",
    "\n",
    "query3 = \"What were the key trends from my past queries?\"\n",
    "response3 = agent_executor.invoke({\"input\": query3})\n",
    "print(response3[\"output\"])  # Provides a summary of past discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_tool` with `{'query': 'ICC T20 world cup champions'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'source': 'https://currentaffairs.adda247.com/t20-cricket-world-cup-winners-list/', 'title': ''}, page_content=\"ICC T20 World Cup Winners List. The T20 World Cup 2022 has concluded, and England emerged as the new champions of the ICC Men's T20 World Cup.\"), Document(metadata={'source': 'https://www.icc-cricket.com/', 'title': ''}, page_content=\"Get all the latest updates and exclusive content on ICC Women's Under-19 T20 World Cup 2025 ICC Women's Under-19 T20 World Cup, 2025 Dominant India secure back to back titles | Final | Match Highlights | U19WC2025 7d ICC Champions Trophy, 2025Every ICC Men's Champions Trophy 2025 squad 1h Tom Banton 11/11/1998England batter makes a return after almost five years for final India ODI 2h ICC Champions Trophy, 2025South Africa name replacement for Nortje ahead of Champions Trophy 2025 2h ICC World Test ChampionshipFinal WTC25 standings confirmed as Australia end their campaign on a high 3h ICC Champions Trophy, 2025Injury concerns hit Pakistan, New Zealand after first tri-series clash with Champions Trophy looming 16h NewsBatting coach updates on Virat Kohli’s availability for second England ODI 21h\"), Document(metadata={'source': 'https://www.careerpower.in/t20-cricket-world-cup-winners-list.html', 'title': ''}, page_content=\"India became champion of the ICC Men's T20 World Cup 2024 by defeating SA by 7 runs. With this, India won its second ICC T20 World Cup title.\")]\u001b[0m\u001b[32;1m\u001b[1;3mThe ICC T20 World Cup champions are England, as they won the most recent tournament in 2022. Prior to that, India won the championship in 2024 by defeating South Africa by 7 runs. These are the only two teams that have won the ICC Men's T20 World Cup, as of the information available up to 2025.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Insights**:\n",
      "The ICC T20 World Cup champions are England, as they won the most recent tournament in 2022. Prior to that, India won the championship in 2024 by defeating South Africa by 7 runs. These are the only two teams that have won the ICC Men's T20 World Cup, as of the information available up to 2025.\n"
     ]
    }
   ],
   "source": [
    "### Import Necessary LangChain Components\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "import getpass\n",
    "\n",
    "\n",
    "API_KEY =\"YL41PNDL63AAWZOI\"\n",
    "\n",
    "@tool(\"stock_api_tool\", return_direct=False)\n",
    "def get_stock_data(stock_symbol: str, data_type: str = \"intraday\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches stock data for a given stock symbol.\n",
    "    \n",
    "    Available data_type options:\n",
    "    - \"intraday\": Latest stock price (1-minute interval)\n",
    "    - \"daily\": Daily adjusted closing prices\n",
    "    - \"fundamental\": Company overview (market cap, EPS, PE ratio)\n",
    "    - \"indicators\": Technical indicators (RSI, MACD)\n",
    "    \"\"\"\n",
    "   \n",
    "    BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "    params = {\"symbol\": stock_symbol, \"apikey\": API_KEY}\n",
    "\n",
    "    if data_type == \"intraday\":\n",
    "        params[\"function\"] = \"TIME_SERIES_INTRADAY\"\n",
    "        params[\"interval\"] = \"1min\"\n",
    "    elif data_type == \"daily\":\n",
    "        params[\"function\"] = \"TIME_SERIES_DAILY_ADJUSTED\"\n",
    "    elif data_type == \"fundamental\":\n",
    "        params[\"function\"] = \"OVERVIEW\"\n",
    "    elif data_type == \"indicators\":\n",
    "        params[\"function\"] = \"RSI\"\n",
    "        params[\"interval\"] = \"daily\"\n",
    "        params[\"time_period\"] = \"14\"\n",
    "        params[\"series_type\"] = \"close\"\n",
    "    elif data_type == \"financials\":\n",
    "        params[\"function\"] = \"INCOME_STATEMENT\"\n",
    "    else:\n",
    "        return \"Invalid data type. Choose from 'intraday', 'daily', 'fundamental', 'financials','market_sentiment_news', or 'indicators'.\"\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    try:\n",
    "        if data_type == \"intraday\":\n",
    "            latest_data = data['Time Series (1min)']\n",
    "            latest_timestamp = next(iter(latest_data))\n",
    "            stock_info = latest_data[latest_timestamp]\n",
    "            return f\"Stock: {stock_symbol} - Open: {stock_info['1. open']}, High: {stock_info['2. high']}, Low: {stock_info['3. low']}, Close: {stock_info['4. close']}, Volume: {stock_info['5. volume']} at {latest_timestamp}\"\n",
    "        \n",
    "        elif data_type == \"daily\":\n",
    "            latest_data = data['Time Series (Daily)']\n",
    "            latest_date = next(iter(latest_data))\n",
    "            stock_info = latest_data[latest_date]\n",
    "            return f\"Stock: {stock_symbol} - Open: {stock_info['1. open']}, High: {stock_info['2. high']}, Low: {stock_info['3. low']}, Close: {stock_info['4. close']}, Adjusted Close: {stock_info['5. adjusted close']}, Volume: {stock_info['6. volume']} on {latest_date}\"\n",
    "        \n",
    "        elif data_type == \"fundamental\":\n",
    "            return f\"Company: {data['Name']} ({stock_symbol})\\nMarket Cap: {data['MarketCapitalization']}\\nEPS: {data['EPS']}\\nPE Ratio: {data['PERatio']}\\nDividend Yield: {data['DividendYield']}\\nSector: {data['Sector']}\"\n",
    "        \n",
    "        elif data_type == \"indicators\":\n",
    "            rsi_data = data['Technical Analysis: RSI']\n",
    "            latest_date = next(iter(rsi_data))\n",
    "            return f\"Stock: {stock_symbol} - RSI: {rsi_data[latest_date]['RSI']} on {latest_date}\"\n",
    "        elif data_type == \"financials\":\n",
    "            annual_reports = data[\"annualReports\"][0]  # Latest financial year report\n",
    "            return f\"Company: {stock_symbol}\\nRevenue: {annual_reports['totalRevenue']}\\nNet Income: {annual_reports['netIncome']}\\nProfit Margin: {annual_reports['grossProfit']}\"\n",
    "\n",
    "\n",
    "    except KeyError:\n",
    "        return \"Error fetching stock data. Check API limits or verify the stock symbol.\"\n",
    "    \n",
    "@tool(\"stock_news_api_tool\", return_direct=False)\n",
    "def get_market_sentiment_news(ticker: str = None, topics: str = None):\n",
    "    \"\"\"\n",
    "    Fetches the latest market sentiment news for a given stock ticker or market topics.\n",
    "\n",
    "    Parameters:\n",
    "    - ticker (str, optional): Stock symbol (e.g., \"AAPL\") to fetch news for.\n",
    "    - topics (str, optional): Market topics such as \"Earnings\", \"IPO\", etc.\n",
    "\n",
    "    Returns:\n",
    "    - str: Formatted string containing the top 3 news articles with title, source, sentiment, and URL.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"apikey\": API_KEY\n",
    "    }\n",
    "\n",
    "    if ticker:\n",
    "        params[\"tickers\"] = ticker  # Fetch news specific to the stock\n",
    "    if topics:\n",
    "        params[\"topics\"] = topics  # Filter by topics like Earnings, IPO, etc.\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    news_items = data.get(\"feed\", [])\n",
    "    if not news_items:\n",
    "        return \"No market news available.\"\n",
    "\n",
    "    # Extract top 3 news articles\n",
    "    news_summary = []\n",
    "    for news in news_items[:3]:\n",
    "        news_summary.append(\n",
    "            f\"**Title**: {news['title']}\\n\"\n",
    "            f\"**Source**: {news['source']}\\n\"\n",
    "            f\"**Sentiment**: {news['overall_sentiment_label']}\\n\"\n",
    "            f\"**URL**: {news['url']}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(news_summary)\n",
    "\n",
    "@tool(\"process_search_tool\", return_direct=False)\n",
    "def process_search_tool(url: str) -> str:\n",
    "    \"\"\"Fetches and extracts content from a given URL.\"\"\"\n",
    "    try:\n",
    "        # Add headers to mimic a real browser\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=5)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract text from specific tags to avoid noise\n",
    "        text = \" \".join([p.get_text() for p in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"article\"])])\n",
    "        return text\n",
    "    except requests.Timeout:\n",
    "        return f\"Timeout error while processing URL {url}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing URL {url}: {str(e)}\"\n",
    "\n",
    "# Asynchronous function to process multiple URLs concurrently\n",
    "async def process_multiple_urls(urls):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # Use partial to pass the function and its arguments\n",
    "    tasks = [loop.run_in_executor(None, partial(process_search_tool, url)) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to fetch and return up to 5 search results from Tavily\n",
    "@tool(\"tavily_search_tool\", return_direct=False)\n",
    "def tavily_rag_search(query: str) -> list:\n",
    "    \"\"\"Enhanced search with RAG processing\"\"\"\n",
    "    tavily = TavilySearchResults(\n",
    "        max_results=5,\n",
    "        search_depth=\"advanced\",\n",
    "        include_answer=True,\n",
    "        include_raw_content=True\n",
    "    )\n",
    "    results = tavily.run(query)\n",
    "# Convert to LangChain Documents\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=res[\"content\"],\n",
    "            metadata={\"source\": res[\"url\"], \"title\": res.get(\"title\", \"\")}\n",
    "        ) for res in results\n",
    "    ]\n",
    "    \n",
    "    # Rest of your RAG processing\n",
    "    search_contents = [doc.page_content for doc in documents]\n",
    "    query_embedding = sbert_model.encode([query])[0]\n",
    "    search_embeddings = sbert_model.encode(search_contents)\n",
    "    \n",
    "    index = faiss.IndexFlatL2(query_embedding.shape[0])\n",
    "    index.add(np.array(search_embeddings))\n",
    "    \n",
    "    k = 3\n",
    "    D, I = index.search(np.array([query_embedding]), k)\n",
    "    \n",
    "    relevant_docs = [documents[i] for i in I[0]]  # Return Document objects\n",
    "    \n",
    "    return relevant_docs\n",
    "\n",
    "# NewsAPI tool to fetch news articles\n",
    "@tool(\"news_api_tool\", return_direct=False)\n",
    "def get_news_from_newsapi(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the latest news articles from NewsAPI for a specific query.\n",
    "    \"\"\"\n",
    "    API_KEY = \"d2afe10169b44e628b2131aed04ac7e4\"  # Add your NewsAPI key here\n",
    "    BASE_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,  # Use the query from Tavily search\n",
    "        \"apiKey\": API_KEY,\n",
    "        \"language\": \"en\",  # You can adjust the language as needed\n",
    "        \"sortBy\": \"relevance\",  # Sort by relevance or any other criteria\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract top 3 news articles\n",
    "    if data.get(\"status\") == \"ok\":\n",
    "        articles = data.get(\"articles\", [])\n",
    "        if articles:\n",
    "            news_summary = []\n",
    "            for article in articles[:3]:  # Limit to top 3 articles\n",
    "                news_summary.append(\n",
    "                    f\"**Title**: {article['title']}\\n\"\n",
    "                    f\"**Source**: {article['source']['name']}\\n\"\n",
    "                    f\"**Description**: {article['description']}\\n\"\n",
    "                    f\"**URL**: {article['url']}\\n\"\n",
    "                )\n",
    "            return \"\\n\".join(news_summary)\n",
    "    return \"No news articles found.\"\n",
    "\n",
    "\n",
    "@tool(\"get_stock_analysis_tool\", return_direct=False)\n",
    "def get_stock_analysis(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches stock financial data, technical indicators, and news sentiment analysis \n",
    "    for a given stock symbol and provides a Buy/Hold/Sell recommendation.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch stock overview (P/E ratio, market cap, dividend yield)\n",
    "        overview_url = f\"https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={API_KEY}\"\n",
    "        stock_data = requests.get(overview_url).json()\n",
    "\n",
    "        if \"Error Message\" in stock_data or \"Note\" in stock_data:\n",
    "            return f\"❌ Error: Could not fetch data for {symbol}. API limit may be reached.\"\n",
    "\n",
    "        pe_ratio = float(stock_data.get(\"PERatio\", 0))\n",
    "        market_cap = float(stock_data.get(\"MarketCapitalization\", 0))\n",
    "        dividend_yield = float(stock_data.get(\"DividendYield\", 0))\n",
    "\n",
    "        # Fetch RSI (Relative Strength Index)\n",
    "        rsi_url = f\"https://www.alphavantage.co/query?function=RSI&symbol={symbol}&interval=daily&time_period=14&series_type=close&apikey={API_KEY}\"\n",
    "        rsi_data = requests.get(rsi_url).json()\n",
    "        rsi_values = rsi_data.get(\"Technical Analysis: RSI\", {})\n",
    "\n",
    "        # Get latest available RSI value\n",
    "        latest_rsi_date = next(iter(rsi_values), None)\n",
    "        rsi_value = float(rsi_values[latest_rsi_date][\"RSI\"]) if latest_rsi_date else 50  # Default 50 if unavailable\n",
    "\n",
    "        # Fetch latest stock price\n",
    "        price_url = f\"https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey={API_KEY}\"\n",
    "        price_data = requests.get(price_url).json()\n",
    "        stock_price = float(price_data.get(\"Global Quote\", {}).get(\"05. price\", 0))\n",
    "\n",
    "        # Fetch News Sentiment\n",
    "        news_url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={API_KEY}\"\n",
    "        news_response = requests.get(news_url).json()\n",
    "        news_sentiment = news_response.get(\"feed\", [])\n",
    "\n",
    "        # Compute average sentiment score from top 5 news articles\n",
    "        total_sentiment = sum(news.get(\"overall_sentiment_score\", 0) for news in news_sentiment[:5])\n",
    "        avg_sentiment = total_sentiment / max(len(news_sentiment[:5]), 1)  # Avoid division by zero\n",
    "\n",
    "        # **Decision Logic**\n",
    "        if rsi_value < 30 and pe_ratio < 20 and avg_sentiment > 0:\n",
    "            recommendation = \"🔵 BUY: The stock is undervalued and news sentiment is positive.\"\n",
    "        elif rsi_value > 70 and avg_sentiment < 0:\n",
    "            recommendation = \"🔴 SELL: The stock is overbought and news sentiment is negative.\"\n",
    "        else:\n",
    "            recommendation = \"🟡 HOLD: Market conditions are stable.\"\n",
    "\n",
    "        return f\"\"\"\n",
    "        📈 **Stock Analysis for {symbol}**\n",
    "        - **Current Price:** ${stock_price:.2f}\n",
    "        - **P/E Ratio:** {pe_ratio:.2f}\n",
    "        - **Market Cap:** ${market_cap:,.0f}\n",
    "        - **RSI (14-day):** {rsi_value:.2f}  \n",
    "        - **Dividend Yield:** {dividend_yield:.2%}\n",
    "        - **News Sentiment Score:** {avg_sentiment:.2f}\n",
    "\n",
    "        **Recommendation: {recommendation}**\n",
    "        \"\"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error fetching data: {str(e)}\"\n",
    "\n",
    "# Define specialized agents\n",
    "def create_stock_data_agent(llm):\n",
    "    tools = [get_stock_data, get_stock_analysis]\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a stock data expert. Fetch and analyze stock data.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),  # Add agent_scratchpad placeholder\n",
    "    ])\n",
    "    return create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "def create_sentiment_agent(llm):\n",
    "    tools = [get_market_sentiment_news]\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a sentiment analysis expert. Analyze news sentiment.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),  # Add agent_scratchpad placeholder\n",
    "    ])\n",
    "    return create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "\n",
    "def create_insights_agent(llm):\n",
    "    tools = [tavily_rag_search,process_search_tool]\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an insights generator. Provide detailed insights.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),  # Add agent_scratchpad placeholder\n",
    "    ])\n",
    "    return create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "def create_general_purpose_agent(llm):\n",
    "    tools = [tavily_rag_search]  # General-purpose tool like Tavily\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a general-purpose assistant. Answer any query comprehensively.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ])\n",
    "    return create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "\n",
    "# Coordinator agent\n",
    "def create_coordinator_agent(llm):\n",
    "    tools = []  # No tools needed for the coordinator\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are the coordinator. Manage interactions between agents and combine their responses.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),  # Add agent_scratchpad placeholder\n",
    "    ])\n",
    "    return create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Initialize agents (replace `llm` with your actual LLM instance)\n",
    "stock_data_agent = create_stock_data_agent(llm)\n",
    "sentiment_agent = create_sentiment_agent(llm)\n",
    "insights_agent = create_insights_agent(llm)\n",
    "coordinator_agent = create_coordinator_agent(llm)\n",
    "\n",
    "# Agent executors\n",
    "stock_data_executor = AgentExecutor(agent=stock_data_agent, tools=[get_stock_data, get_stock_analysis], verbose=True)\n",
    "sentiment_executor = AgentExecutor(agent=sentiment_agent, tools=[get_market_sentiment_news], verbose=True)\n",
    "insights_executor = AgentExecutor(agent=insights_agent, tools=[tavily_rag_search,process_search_tool], verbose=True)\n",
    "coordinator_executor = AgentExecutor(agent=coordinator_agent, tools=[], verbose=True)\n",
    "\n",
    "\n",
    "def classify_query(query):\n",
    "    \"\"\"Classifies the query type: stock/finance, sentiment, or general-purpose.\n",
    "    Handles cases where the query contains keywords from multiple categories.\"\"\"\n",
    "    stock_keywords = [\"stock\", \"market\", \"share\", \"nasdaq\", \"dow jones\", \"finance\", \"investment\"]\n",
    "    sentiment_keywords = [\"sentiment\", \"news\", \"social media\", \"opinion\", \"trends\"]\n",
    "    \n",
    "    is_stock = any(keyword in query.lower() for keyword in stock_keywords)\n",
    "    is_sentiment = any(keyword in query.lower() for keyword in sentiment_keywords)\n",
    "    \n",
    "    if is_stock and is_sentiment:\n",
    "        return \"both\"  # Handle cases where the query is relevant to both categories\n",
    "    elif is_stock:\n",
    "        return \"stock\"\n",
    "    elif is_sentiment:\n",
    "        return \"sentiment\"\n",
    "    else:\n",
    "        return \"general\"  # Default to Tavily for general queries\n",
    "    \n",
    "    \n",
    "def generate_insights_prompt(query, query_type):\n",
    "    \"\"\"Generates a context-aware prompt for the insights agent based on the query type.\"\"\"\n",
    "    if query_type in [\"stock\", \"both\"]:\n",
    "        return f\"Generate a financial analysis and investment insights for {query}. Consider earnings reports, revenue trends, P/E ratio, and market positioning.\"\n",
    "    elif query_type == \"sentiment\":\n",
    "        return f\"Provide insights based on the sentiment analysis for {query}. Summarize key trends, opinions, and potential implications.\"\n",
    "    elif query_type == \"general\":\n",
    "        return f\"Provide detailed insights and analysis for {query}. Consider relevant facts, trends, and context.\"\n",
    "    else:\n",
    "        return f\"Provide insights and analysis for {query}.\"\n",
    "\n",
    "        \n",
    "def multi_agent_query(query):\n",
    "    responses = []\n",
    "    errors = []\n",
    "    query_type = classify_query(query)\n",
    "    print(query_type)\n",
    "\n",
    "\n",
    "    # Fetch stock data if it's a stock-related query\n",
    "    if query_type in [\"stock\", \"both\"]:\n",
    "        try:\n",
    "            stock_data_response = stock_data_executor.invoke(\n",
    "                {\"input\": f\"Retrieve the latest stock data and market trends for {query}. Provide key statistics, including open, high, low, close, and volume.\"}\n",
    "            )\n",
    "            responses.append(f\"**Stock Data Analysis**:\\n{stock_data_response['output']}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"❌ Stock Data Agent failed: {str(e)}\")\n",
    "\n",
    "    # Fetch sentiment analysis if it's related to financial sentiment\n",
    "    if query_type in [\"sentiment\", \"both\"]:\n",
    "        try:\n",
    "            sentiment_response = sentiment_executor.invoke(\n",
    "                {\"input\": f\"Analyze the market sentiment for {query}. Summarize the tone of recent news articles, social media discussions, and investor opinions.\"}\n",
    "            )\n",
    "            responses.append(f\"**Sentiment Analysis**:\\n{sentiment_response['output']}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"❌ Sentiment Agent failed: {str(e)}\")\n",
    "            \n",
    "    # Step 3: Generate insights (generic input)\n",
    "    if query_type in [\"stock\", \"sentiment\", \"both\", \"general\"]:\n",
    "        try:\n",
    "            insights_prompt = generate_insights_prompt(query, query_type)\n",
    "            insights_response = insights_executor.invoke({\"input\": insights_prompt})\n",
    "            responses.append(f\"**Insights**:\\n{insights_response['output']}\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"❌ Insights Agent failed: {str(e)}\")\n",
    "\n",
    "    # Combine responses\n",
    "    final_response = \"\\n\\n\".join(responses) if responses else \"No data available.\"\n",
    "\n",
    "    # Append errors if any agents failed\n",
    "    if errors:\n",
    "        final_response += \"\\n\\n**Errors**:\\n\" + \"\\n\".join(errors)\n",
    "\n",
    "    return final_response\n",
    "\n",
    "\n",
    "# Example\n",
    "query = \"Who are the current ICC T20 world cup champions?\"\n",
    "response = multi_agent_query(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
